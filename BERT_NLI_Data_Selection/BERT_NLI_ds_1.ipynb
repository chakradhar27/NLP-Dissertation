{"nbformat":4,"nbformat_minor":0,"metadata":{"accelerator":"GPU","colab":{"name":"BERT_NLI_ds_1.ipynb","provenance":[],"collapsed_sections":[],"machine_shape":"hm"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"PJln9ooqhDRc","executionInfo":{"status":"ok","timestamp":1630672035928,"user_tz":-60,"elapsed":25369,"user":{"displayName":"varma","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg0mykgB2BlfeZ8lmORSdRPDRvODaQ8LpLMBHSi3Q=s64","userId":"17507925689662955290"}},"outputId":"30c740d7-5bb4-4837-f673-0e518b41454b"},"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"markdown","metadata":{"id":"3ncD6OYJPlIU"},"source":["# BERT"]},{"cell_type":"code","metadata":{"id":"XpHRqfgAPrpo"},"source":["import torch\n","import random\n","import pandas as pd\n","import numpy as np\n","from sklearn.model_selection import train_test_split\n","from sklearn.metrics import accuracy_score, confusion_matrix, classification_report"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"7rt8gLkdjAd2"},"source":["SEED = 1111\n","random.seed(SEED)\n","np.random.seed(SEED)\n","torch.manual_seed(SEED)\n","torch.backends.cudnn.deterministic = True"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"8Y7-YUp5P6ac","executionInfo":{"status":"ok","timestamp":1630672986798,"user_tz":-60,"elapsed":2798,"user":{"displayName":"varma","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg0mykgB2BlfeZ8lmORSdRPDRvODaQ8LpLMBHSi3Q=s64","userId":"17507925689662955290"}},"outputId":"36ae80a9-92e6-42a6-fc24-551088a8b643"},"source":["!pip install transformers"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: transformers in /usr/local/lib/python3.7/dist-packages (4.10.0)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n","Requirement already satisfied: sacremoses in /usr/local/lib/python3.7/dist-packages (from transformers) (0.0.45)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from transformers) (21.0)\n","Requirement already satisfied: tokenizers<0.11,>=0.10.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.10.3)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.19.5)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (5.4.1)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.0.12)\n","Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n","Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.6.4)\n","Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.62.0)\n","Requirement already satisfied: huggingface-hub>=0.0.12 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.0.16)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from huggingface-hub>=0.0.12->transformers) (3.7.4.3)\n","Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->transformers) (2.4.7)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.5.0)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2021.5.30)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n","Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.0.1)\n","Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (7.1.2)\n","Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.15.0)\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"PPGBBNzlQS41","executionInfo":{"status":"ok","timestamp":1630672991218,"user_tz":-60,"elapsed":4424,"user":{"displayName":"varma","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg0mykgB2BlfeZ8lmORSdRPDRvODaQ8LpLMBHSi3Q=s64","userId":"17507925689662955290"}},"outputId":"2fff41ab-046a-4e43-fa38-b77683091c6a"},"source":["from transformers import BertTokenizer\n","tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n","len(tokenizer)"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["30522"]},"metadata":{},"execution_count":4}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"pvJM6v8TQdy0","executionInfo":{"status":"ok","timestamp":1630672991219,"user_tz":-60,"elapsed":38,"user":{"displayName":"varma","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg0mykgB2BlfeZ8lmORSdRPDRvODaQ8LpLMBHSi3Q=s64","userId":"17507925689662955290"}},"outputId":"88d893e4-dc0d-4b9d-f1c9-097efa6e2881"},"source":["cls_token = tokenizer.cls_token\n","sep_token = tokenizer.sep_token\n","pad_token = tokenizer.pad_token\n","unk_token = tokenizer.unk_token\n","print(cls_token, sep_token, pad_token, unk_token)"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["[CLS] [SEP] [PAD] [UNK]\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"FKVH6P38Qjwf","executionInfo":{"status":"ok","timestamp":1630672991219,"user_tz":-60,"elapsed":35,"user":{"displayName":"varma","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg0mykgB2BlfeZ8lmORSdRPDRvODaQ8LpLMBHSi3Q=s64","userId":"17507925689662955290"}},"outputId":"001a1aa3-0174-4b86-9802-97f6f0ba3be2"},"source":["cls_token_idx = tokenizer.cls_token_id\n","sep_token_idx = tokenizer.sep_token_id\n","pad_token_idx = tokenizer.pad_token_id\n","unk_token_idx = tokenizer.unk_token_id\n","print(cls_token_idx, sep_token_idx, pad_token_idx, unk_token_idx)"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["101 102 0 100\n"]}]},{"cell_type":"code","metadata":{"id":"GReKW8obQlDF"},"source":["max_input_length = 512\n","\n","def tokenize_bert(sentence):\n","    tokens = tokenizer.tokenize(sentence) \n","    return tokens\n","def split_and_cut(sentence):\n","    tokens = sentence.strip().split(\" \")\n","    tokens = tokens[:max_input_length]\n","    return tokens\n","\n","def trim_sentence(sent):\n","    try:\n","        sent = sent.split()\n","        sent = sent[:256]\n","        return \" \".join(sent)\n","    except:\n","        return sent"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"mQyx-wZ6Qtmg"},"source":["def get_sent1_token_type(sent):\n","    try:\n","        return [0]* len(sent)\n","    except:\n","        return []\n","#Get list of 1s\n","def get_sent2_token_type(sent):\n","    try:\n","        return [1]* len(sent)\n","    except:\n","        return []\n","#combine from lists\n","def combine_seq(seq):\n","    return \" \".join(seq)\n","#combines from lists of int\n","def combine_mask(mask):\n","    mask = [str(m) for m in mask]\n","    return \" \".join(mask)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"oKQG3hJEhgqK"},"source":["##**Data Preparation**"]},{"cell_type":"code","metadata":{"id":"JQBz13qsVnfq"},"source":["#convert jsonl from DINO to csv/tsv\n","#df = pd.read_json('/content/drive/MyDrive/NLP_Dissertation/DINO/output/nc-nli-1-combined/sts-nc-nli-comb-dataset-pp.jsonl', lines=True)\n","##df.head()\n","#df = df.rename({\"text_a\":\"sentence1\",\"text_b\":\"sentence2\"}, axis='columns')\n","#df['gold_label'] = np.where(df['label'] > 0.5, 'entailment', (np.where(df['label'] == 0.5, 'neutral', 'contradiction')))\n","##df = df.sort_values('index')\n","#df.to_csv('/content/drive/MyDrive/NLP_Dissertation/BERT_NLI_Data_Selection/Dataset/nc-nli-1/nc-nli-1.csv', index=False)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"gA74svnsXNgS"},"source":["#split the dataset\n","##df_train, df_dev, df_test = np.split(df.sample(frac=1, random_state=27), [int(.7*len(df)), int(.85*len(df))])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"NxI_f4Luk0sz","executionInfo":{"status":"ok","timestamp":1630672991222,"user_tz":-60,"elapsed":29,"user":{"displayName":"varma","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg0mykgB2BlfeZ8lmORSdRPDRvODaQ8LpLMBHSi3Q=s64","userId":"17507925689662955290"}},"outputId":"15ca0b32-a508-4a72-cca2-c51831521988"},"source":["df = pd.read_csv(\"/content/drive/MyDrive/NLP_Dissertation/BERT_NLI_Data_Selection/Dataset/nc-nli-1/nc-nli-1.csv\")\n","df.info()"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["<class 'pandas.core.frame.DataFrame'>\n","RangeIndex: 3906 entries, 0 to 3905\n","Data columns (total 5 columns):\n"," #   Column      Non-Null Count  Dtype  \n","---  ------      --------------  -----  \n"," 0   sentence1   3906 non-null   object \n"," 1   sentence2   3906 non-null   object \n"," 2   label       3906 non-null   float64\n"," 3   index       3906 non-null   int64  \n"," 4   gold_label  3906 non-null   object \n","dtypes: float64(1), int64(1), object(3)\n","memory usage: 152.7+ KB\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"W1xiEgYOk5CA","executionInfo":{"status":"ok","timestamp":1630672991223,"user_tz":-60,"elapsed":25,"user":{"displayName":"varma","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg0mykgB2BlfeZ8lmORSdRPDRvODaQ8LpLMBHSi3Q=s64","userId":"17507925689662955290"}},"outputId":"2de53d6c-86a3-46da-ea32-478c36b98b97"},"source":["df['gold_label'].value_counts()"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["entailment       1329\n","neutral          1316\n","contradiction    1261\n","Name: gold_label, dtype: int64"]},"metadata":{},"execution_count":12}]},{"cell_type":"code","metadata":{"id":"7doyx5uzzGWE"},"source":["df_train_set, df_test = train_test_split(df, test_size=0.15, random_state=27, stratify=df['gold_label'])\n","df_train, df_dev = train_test_split(df_train_set, test_size=0.15, random_state=27, stratify=df_train_set['gold_label'])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"kg7Rjwq9QvMS"},"source":["#Get neccesary columns\n","df_train = df_train[['gold_label','sentence1','sentence2']]\n","df_dev = df_dev[['gold_label','sentence1','sentence2']]\n","df_test = df_test[['gold_label','sentence1','sentence2']]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"mVDnDttPP-0x"},"source":["#Trim each sentence upto maximum length\n","df_train['sentence1'] = df_train['sentence1'].apply(trim_sentence)\n","df_train['sentence2'] = df_train['sentence2'].apply(trim_sentence)\n","df_dev['sentence1'] = df_dev['sentence1'].apply(trim_sentence)\n","df_dev['sentence2'] = df_dev['sentence2'].apply(trim_sentence)\n","df_test['sentence1'] = df_test['sentence1'].apply(trim_sentence)\n","df_test['sentence2'] = df_test['sentence2'].apply(trim_sentence)\n","\n","#Add [CLS] and [SEP] tokens\n","df_train['sent1'] = '[CLS] ' + df_train['sentence1'] + ' [SEP] '\n","df_train['sent2'] = df_train['sentence2'] + ' [SEP]'\n","df_dev['sent1'] = '[CLS] ' + df_dev['sentence1'] + ' [SEP] '\n","df_dev['sent2'] = df_dev['sentence2'] + ' [SEP]'\n","df_test['sent1'] = '[CLS] ' + df_test['sentence1'] + ' [SEP] '\n","df_test['sent2'] = df_test['sentence2'] + ' [SEP]'\n","\n","#Apply Bert Tokenizer for tokeinizing\n","df_train['sent1_t'] = df_train['sent1'].apply(tokenize_bert)\n","df_train['sent2_t'] = df_train['sent2'].apply(tokenize_bert)\n","df_dev['sent1_t'] = df_dev['sent1'].apply(tokenize_bert)\n","df_dev['sent2_t'] = df_dev['sent2'].apply(tokenize_bert)\n","df_test['sent1_t'] = df_test['sent1'].apply(tokenize_bert)\n","df_test['sent2_t'] = df_test['sent2'].apply(tokenize_bert)\n","\n","\n","#Get Token type ids for both sentence\n","df_train['sent1_token_type'] = df_train['sent1_t'].apply(get_sent1_token_type)\n","df_train['sent2_token_type'] = df_train['sent2_t'].apply(get_sent2_token_type)\n","df_dev['sent1_token_type'] = df_dev['sent1_t'].apply(get_sent1_token_type)\n","df_dev['sent2_token_type'] = df_dev['sent2_t'].apply(get_sent2_token_type)\n","df_test['sent1_token_type'] = df_test['sent1_t'].apply(get_sent1_token_type)\n","df_test['sent2_token_type'] = df_test['sent2_t'].apply(get_sent2_token_type)\n","\n","#Combine both sequences\n","df_train['sequence'] = df_train['sent1_t'] + df_train['sent2_t']\n","df_dev['sequence'] = df_dev['sent1_t'] + df_dev['sent2_t']\n","df_test['sequence'] = df_test['sent1_t'] + df_test['sent2_t']\n","\n","\n","#Get attention mask\n","df_train['attention_mask'] = df_train['sequence'].apply(get_sent2_token_type)\n","df_dev['attention_mask'] = df_dev['sequence'].apply(get_sent2_token_type)\n","df_test['attention_mask'] = df_test['sequence'].apply(get_sent2_token_type)\n","\n","#Get combined token type ids for input\n","df_train['token_type'] = df_train['sent1_token_type'] + df_train['sent2_token_type']\n","df_dev['token_type'] = df_dev['sent1_token_type'] + df_dev['sent2_token_type']\n","df_test['token_type'] = df_test['sent1_token_type'] + df_test['sent2_token_type']\n","\n","#Now make all these inputs as sequential data to be easily fed into torchtext Field.\n","df_train['sequence'] = df_train['sequence'].apply(combine_seq)\n","df_dev['sequence'] = df_dev['sequence'].apply(combine_seq)\n","df_test['sequence'] = df_test['sequence'].apply(combine_seq)\n","df_train['attention_mask'] = df_train['attention_mask'].apply(combine_mask)\n","df_dev['attention_mask'] = df_dev['attention_mask'].apply(combine_mask)\n","df_test['attention_mask'] = df_test['attention_mask'].apply(combine_mask)\n","df_train['token_type'] = df_train['token_type'].apply(combine_mask)\n","df_dev['token_type'] = df_dev['token_type'].apply(combine_mask)\n","df_test['token_type'] = df_test['token_type'].apply(combine_mask)\n","df_train = df_train[['gold_label', 'sequence', 'attention_mask', 'token_type']]\n","df_dev = df_dev[['gold_label', 'sequence', 'attention_mask', 'token_type']]\n","df_test = df_test[['gold_label', 'sequence', 'attention_mask', 'token_type']]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"hTCHlszqRFOs"},"source":["df_train = df_train.loc[df_train['gold_label'].isin(['entailment','contradiction','neutral'])]\n","df_dev = df_dev.loc[df_dev['gold_label'].isin(['entailment','contradiction','neutral'])]\n","df_test = df_test.loc[df_test['gold_label'].isin(['entailment','contradiction','neutral'])]"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"R08kKZ_R33Mw"},"source":["**NC-NLI-1**"]},{"cell_type":"code","metadata":{"id":"0Zj1_mTqRPxK"},"source":["df_train.to_csv('/content/drive/MyDrive/NLP_Dissertation/BERT_NLI_Data_Selection/Dataset/nc-nli-1/nli_1.0_train.csv', index=False)\n","df_dev.to_csv('/content/drive/MyDrive/NLP_Dissertation/BERT_NLI_Data_Selection/Dataset/nc-nli-1/nli_1.0_dev.csv', index=False)\n","df_test.to_csv('/content/drive/MyDrive/NLP_Dissertation/BERT_NLI_Data_Selection/Dataset/nc-nli-1/nli_1.0_test.csv', index=False)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"X_bJm0X0RdMK"},"source":["def convert_to_int(tok_ids):\n","    tok_ids = [int(x) for x in tok_ids]\n","    return tok_ids"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Of_dwrNtRhwG"},"source":["from torchtext.legacy import data\n","#For sequence\n","TEXT = data.Field(batch_first = True,\n","                  use_vocab = False,\n","                  tokenize = split_and_cut,\n","                  preprocessing = tokenizer.convert_tokens_to_ids,\n","                  pad_token = pad_token_idx,\n","                  unk_token = unk_token_idx)\n","#For label\n","LABEL = data.LabelField()\n","#For Attention mask\n","ATTENTION = data.Field(batch_first = True,\n","                  use_vocab = False,\n","                  tokenize = split_and_cut,\n","                  preprocessing = convert_to_int,\n","                  pad_token = pad_token_idx)\n","#For token type ids\n","TTYPE = data.Field(batch_first = True,\n","                  use_vocab = False,\n","                  tokenize = split_and_cut,\n","                  preprocessing = convert_to_int,\n","                  pad_token = 1)\n","\n","fields = [('label', LABEL), ('sequence', TEXT), ('attention_mask', ATTENTION), ('token_type', TTYPE)]"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"hc3G-BD23u6e"},"source":["**NC-NLI-1**"]},{"cell_type":"code","metadata":{"id":"eb45HAyFRmpg"},"source":["train_data, valid_data, test_data = data.TabularDataset.splits(\n","                                        path = '/content/drive/MyDrive/NLP_Dissertation/BERT_NLI_Data_Selection/Dataset/nc-nli-1',\n","                                        train = 'nli_1.0_train.csv',\n","                                        validation = 'nli_1.0_dev.csv',\n","                                        test = 'nli_1.0_test.csv',\n","                                        format = 'csv',\n","                                        fields = fields,\n","                                        skip_header = True)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"3m1OCH0GRwz4"},"source":["train_data_len = len(train_data)\n","LABEL.build_vocab(train_data)\n","\n","BATCH_SIZE = 16\n","device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","train_iterator, valid_iterator, test_iterator = data.BucketIterator.splits(\n","    (train_data, valid_data, test_data), \n","    batch_size = BATCH_SIZE,\n","    sort_key = lambda x: len(x.sequence),\n","    sort_within_batch = False, \n","    device = device)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"CTRTHk3_R2lK","executionInfo":{"status":"ok","timestamp":1630672997606,"user_tz":-60,"elapsed":1808,"user":{"displayName":"varma","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg0mykgB2BlfeZ8lmORSdRPDRvODaQ8LpLMBHSi3Q=s64","userId":"17507925689662955290"}},"outputId":"d763f627-6814-4615-b517-88ecc60acd8b"},"source":["from transformers import BertModel\n","bert_model = BertModel.from_pretrained('bert-base-uncased')"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight', 'cls.predictions.decoder.weight']\n","- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"]}]},{"cell_type":"code","metadata":{"id":"5TpGDXiCR-YV"},"source":["import torch.nn as nn\n","class BERTNLIModel(nn.Module):\n","    def __init__(self,\n","\n","                 bert_model,\n","\n","                 hidden_dim,\n","\n","                 output_dim,\n","\n","                ):\n","        super().__init__()\n","        self.bert = bert_model\n","\n","        embedding_dim = bert_model.config.to_dict()['hidden_size']\n","        self.out = nn.Linear(embedding_dim, output_dim)\n","    def forward(self, sequence, attn_mask, token_type):\n","        embedded = self.bert(input_ids = sequence, attention_mask = attn_mask, token_type_ids= token_type)[1]\n","        output = self.out(embedded)\n","        return output"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"WiRT86w3R_Sy"},"source":["HIDDEN_DIM = 512\n","OUTPUT_DIM = len(LABEL.vocab)\n","model = BERTNLIModel(bert_model,\n","                         HIDDEN_DIM,\n","                         OUTPUT_DIM,\n","                        ).to(device)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"a5-kdoQVSEjZ"},"source":["from transformers import AdamW, get_constant_schedule_with_warmup\n","import torch.optim as optim\n","optimizer = AdamW(model.parameters(),lr=2e-5,eps=1e-6,correct_bias=False)\n","def get_scheduler(optimizer, warmup_steps):\n","    scheduler = get_constant_schedule_with_warmup(optimizer, num_warmup_steps=warmup_steps)\n","    return scheduler\n"," \n","criterion = nn.CrossEntropyLoss().to(device)\n","\n","def categorical_accuracy(preds, y):\n","    max_preds = preds.argmax(dim = 1, keepdim = True)\n","\n","    correct = (max_preds.squeeze(1)==y).float()\n","\n","    return correct.sum() / len(y)\n","\n","max_grad_norm = 1\n","\n","def train(model, iterator, optimizer, criterion, scheduler):\n","    #print(iterator)\n","    \n","    epoch_loss = 0\n","    epoch_acc = 0\n","    \n","    model.train()\n","    \n","    for batch in iterator:\n","\n","        optimizer.zero_grad() # clear gradients first\n","        torch.cuda.empty_cache() # releases all unoccupied cached memory \n","        \n","        sequence = batch.sequence\n","        attn_mask = batch.attention_mask\n","        token_type = batch.token_type\n","        label = batch.label\n","        \n","        predictions = model(sequence, attn_mask, token_type)\n","        \n","        loss = criterion(predictions, label)\n","        \n","        acc = categorical_accuracy(predictions, label)\n","        \n","        if mp:\n","            with amp.scale_loss(loss, optimizer) as scaled_loss:\n","                scaled_loss.backward()\n","            torch.nn.utils.clip_grad_norm_(amp.master_params(optimizer), max_grad_norm)\n","        else:\n","            loss.backward()\n","        \n","        optimizer.step()\n","        scheduler.step()\n","        \n","        epoch_loss += loss.item()\n","        epoch_acc += acc.item()\n","        \n","    return epoch_loss / len(iterator), epoch_acc / len(iterator)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Fvhuv0EuSQdh"},"source":["def evaluate(model, iterator, criterion):\n","    #print(iterator)\n","    epoch_loss = 0\n","    epoch_acc = 0\n","    \n","    model.eval()\n","    \n","    with torch.no_grad():\n","    \n","        for batch in iterator:\n","            #print(batch)\n","\n","            sequence = batch.sequence\n","            attn_mask = batch.attention_mask\n","            token_type = batch.token_type\n","            labels = batch.label\n","                        \n","            predictions = model(sequence, attn_mask, token_type)\n","            \n","            loss = criterion(predictions, labels)\n","                \n","            acc = categorical_accuracy(predictions, labels)\n","            \n","            epoch_loss += loss.item()\n","            epoch_acc += acc.item()\n","        \n","    return epoch_loss / len(iterator), epoch_acc / len(iterator)  "],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"021tXT6lSUA2","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1630673145259,"user_tz":-60,"elapsed":144326,"user":{"displayName":"varma","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg0mykgB2BlfeZ8lmORSdRPDRvODaQ8LpLMBHSi3Q=s64","userId":"17507925689662955290"}},"outputId":"ac3c849a-86b8-469a-93d3-388368b030e5"},"source":["import time\n","def epoch_time(start_time, end_time):\n","  elapsed_time = end_time - start_time\n","  elapsed_mins = int(elapsed_time / 60)\n","  elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n","  return elapsed_mins, elapsed_secs\n","\n","import math\n","N_EPOCHS = 3\n","mp = False\n","warmup_percent = 0.2\n","total_steps = math.ceil(N_EPOCHS*train_data_len*1./BATCH_SIZE)\n","warmup_steps = int(total_steps*warmup_percent)\n","scheduler = get_scheduler(optimizer, warmup_steps)\n","\n","best_valid_loss = float('inf')\n","\n","for epoch in range(N_EPOCHS):\n","\n","    start_time = time.time()\n","    \n","    train_loss, train_acc = train(model, train_iterator, optimizer, criterion, scheduler)\n","    valid_loss, valid_acc = evaluate(model, valid_iterator, criterion)\n","    \n","    end_time = time.time()\n","\n","    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n","    \n","    if valid_loss < best_valid_loss:\n","        best_valid_loss = valid_loss\n","        torch.save(model.state_dict(), 'bert-nli.pt')\n","    \n","    print(f'Epoch: {epoch+1:02} | Epoch Time: {epoch_mins}m {epoch_secs}s')\n","    print(f'\\tTrain Loss: {train_loss:.3f} | Train Acc: {train_acc*100:.2f}%')\n","    print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. Acc: {valid_acc*100:.2f}%')"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch: 01 | Epoch Time: 0m 46s\n","\tTrain Loss: 1.040 | Train Acc: 44.44%\n","\t Val. Loss: 0.989 |  Val. Acc: 50.20%\n","Epoch: 02 | Epoch Time: 0m 47s\n","\tTrain Loss: 0.789 | Train Acc: 64.01%\n","\t Val. Loss: 0.886 |  Val. Acc: 59.18%\n","Epoch: 03 | Epoch Time: 0m 48s\n","\tTrain Loss: 0.456 | Train Acc: 81.71%\n","\t Val. Loss: 1.181 |  Val. Acc: 55.47%\n"]}]},{"cell_type":"code","metadata":{"id":"AERN3vyhSfgp","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1630673147489,"user_tz":-60,"elapsed":2240,"user":{"displayName":"varma","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg0mykgB2BlfeZ8lmORSdRPDRvODaQ8LpLMBHSi3Q=s64","userId":"17507925689662955290"}},"outputId":"1a2c28e7-88fe-4095-be2d-849f42a0a264"},"source":["model.load_state_dict(torch.load('bert-nli.pt'))\n","test_loss, test_acc = evaluate(model, test_iterator, criterion)\n","print(f'Test Loss: {test_loss:.3f} |  Test Acc: {test_acc*100:.2f}%')"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Test Loss: 0.924 |  Test Acc: 55.37%\n"]}]},{"cell_type":"code","metadata":{"id":"Sc_lfzofSiVr"},"source":["def predict_inference(premise, hypothesis, model, device):\n","    model.eval()\n","    premise = '[CLS] ' + premise + ' [SEP]'\n","    hypothesis = hypothesis + ' [SEP]'\n","    prem_t = tokenize_bert(premise)\n","    hypo_t = tokenize_bert(hypothesis)\n","    prem_type = get_sent1_token_type(prem_t)\n","    hypo_type = get_sent2_token_type(hypo_t)\n","    indexes = prem_t + hypo_t\n","    indexes = tokenizer.convert_tokens_to_ids(indexes)\n","    indexes_type = prem_type + hypo_type\n","    attn_mask = get_sent2_token_type(indexes)\n","    indexes = torch.LongTensor(indexes).unsqueeze(0).to(device)\n","    indexes_type = torch.LongTensor(indexes_type).unsqueeze(0).to(device)\n","    attn_mask = torch.LongTensor(attn_mask).unsqueeze(0).to(device)\n","    prediction = model(indexes, attn_mask, indexes_type)\n","    prediction = prediction.argmax(dim=-1).item()\n","    return LABEL.vocab.itos[prediction]"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"kfaeFNidkDKs"},"source":["# **evaluation on nli_probe**"]},{"cell_type":"code","metadata":{"id":"8ace3RjXwU1X"},"source":["df_test_pred = pd.read_excel('/content/drive/MyDrive/NLP_Dissertation/Datasets/nli_probe.xlsx')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"si75eTeUwAJd"},"source":["pred = []\n","for i in range(0,len(df_test_pred)): \n","  lbl = predict_inference(df_test_pred['sentence1'][i], df_test_pred['sentence2'][i], model, device)\n","  pred.append(lbl)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Hd5IENyCwAYk","executionInfo":{"status":"ok","timestamp":1630673188983,"user_tz":-60,"elapsed":20,"user":{"displayName":"varma","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg0mykgB2BlfeZ8lmORSdRPDRvODaQ8LpLMBHSi3Q=s64","userId":"17507925689662955290"}},"outputId":"f03694a3-e10d-4f13-f153-825f584c57ee"},"source":["acc = accuracy_score(df_test_pred['label'],pred)\n","acc"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["0.7716417910447761"]},"metadata":{},"execution_count":32}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"sH9tttkxx1GB","executionInfo":{"status":"ok","timestamp":1630673188984,"user_tz":-60,"elapsed":19,"user":{"displayName":"varma","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg0mykgB2BlfeZ8lmORSdRPDRvODaQ8LpLMBHSi3Q=s64","userId":"17507925689662955290"}},"outputId":"f1250e88-e404-4f88-ad2f-98c1f0570b04"},"source":["cm = pd.DataFrame(confusion_matrix(df_test_pred['label'],pred), columns=['contradiction', 'entailment', 'neutral'], \n","                  index=['contradiction', 'entailment', 'neutral'])\n","print(cm)"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["               contradiction  entailment  neutral\n","contradiction              0           0        0\n","entailment                 2         517      151\n","neutral                    0           0        0\n"]}]},{"cell_type":"code","metadata":{"id":"2k3bb-r3x1xM"},"source":["#for i,j in zip(list(df_test_pred['gold_label']), pred):\n","#  print(i,j)\n","\n","pred_data = []\n","for index, (first, second) in enumerate(zip(list(df_test_pred['label']), pred)):\n","    if first != second:\n","        pred_data.append([df_test_pred['sentence1'][index],df_test_pred['sentence2'][index],df_test_pred['label'][index], pred[index]])\n","\n","df_mis = pd.DataFrame(pred_data, columns=['sent1', 'sent2', 'gold_lbl', 'pred_lbl'])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":417},"id":"YHDS7mMyx_YI","executionInfo":{"status":"ok","timestamp":1630673188985,"user_tz":-60,"elapsed":16,"user":{"displayName":"varma","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg0mykgB2BlfeZ8lmORSdRPDRvODaQ8LpLMBHSi3Q=s64","userId":"17507925689662955290"}},"outputId":"d97bd16e-4485-435c-ad31-d0fb642584f5"},"source":["df_mis"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>sent1</th>\n","      <th>sent2</th>\n","      <th>gold_lbl</th>\n","      <th>pred_lbl</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>So... be brave, be ready to change your views....</td>\n","      <td>So... be brave, be ready to change your views....</td>\n","      <td>entailment</td>\n","      <td>neutral</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>We also have a MacLarens baby buggy and pottie...</td>\n","      <td>We also have a MacLarens pram and potties and ...</td>\n","      <td>entailment</td>\n","      <td>neutral</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>However, Whitehall thinks that I am a thorough...</td>\n","      <td>However, Whitehall thinks that I am a thorough...</td>\n","      <td>entailment</td>\n","      <td>neutral</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>Prince came onboard to have a large benign tum...</td>\n","      <td>Prince came onboard to have a large noncancero...</td>\n","      <td>entailment</td>\n","      <td>neutral</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>You may occasionally enjoy a good spirited deb...</td>\n","      <td>You may occasionally enjoy a good spirited deb...</td>\n","      <td>entailment</td>\n","      <td>neutral</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>148</th>\n","      <td>daily , but other information e.g. on the term...</td>\n","      <td>daily , but other information e.g. on the term...</td>\n","      <td>entailment</td>\n","      <td>neutral</td>\n","    </tr>\n","    <tr>\n","      <th>149</th>\n","      <td>Arts Employment How to Apply Entry only by app...</td>\n","      <td>Arts Employment How to Apply Entry only by app...</td>\n","      <td>entailment</td>\n","      <td>neutral</td>\n","    </tr>\n","    <tr>\n","      <th>150</th>\n","      <td>documents A : Plans , Decision Notices ( compl...</td>\n","      <td>documents A : Plans , Decision Notices ( compl...</td>\n","      <td>entailment</td>\n","      <td>neutral</td>\n","    </tr>\n","    <tr>\n","      <th>151</th>\n","      <td>feel as if this baby is more important than th...</td>\n","      <td>feel as if this baby is more important than th...</td>\n","      <td>entailment</td>\n","      <td>neutral</td>\n","    </tr>\n","    <tr>\n","      <th>152</th>\n","      <td>to Terry ' . The playlets are punctuated with ...</td>\n","      <td>to Terry ' . The playlets are punctuated with ...</td>\n","      <td>entailment</td>\n","      <td>neutral</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>153 rows × 4 columns</p>\n","</div>"],"text/plain":["                                                 sent1  ... pred_lbl\n","0    So... be brave, be ready to change your views....  ...  neutral\n","1    We also have a MacLarens baby buggy and pottie...  ...  neutral\n","2    However, Whitehall thinks that I am a thorough...  ...  neutral\n","3    Prince came onboard to have a large benign tum...  ...  neutral\n","4    You may occasionally enjoy a good spirited deb...  ...  neutral\n","..                                                 ...  ...      ...\n","148  daily , but other information e.g. on the term...  ...  neutral\n","149  Arts Employment How to Apply Entry only by app...  ...  neutral\n","150  documents A : Plans , Decision Notices ( compl...  ...  neutral\n","151  feel as if this baby is more important than th...  ...  neutral\n","152  to Terry ' . The playlets are punctuated with ...  ...  neutral\n","\n","[153 rows x 4 columns]"]},"metadata":{},"execution_count":35}]},{"cell_type":"markdown","metadata":{"id":"iaUwkqSUkN0b"},"source":["# **evaluation on nli_pet**"]},{"cell_type":"code","metadata":{"id":"6L44K9xvkN0b"},"source":["df_test_pred = pd.read_csv('/content/drive/MyDrive/NLP_Dissertation/Datasets/nli_pet.csv')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ao6YRaickN0b"},"source":["pred = []\n","for i in range(0,len(df_test_pred)): \n","  lbl = predict_inference(df_test_pred['sentence1'][i], df_test_pred['sentence2'][i], model, device)\n","  pred.append(lbl)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"oAPLsQZXkN0c","executionInfo":{"status":"ok","timestamp":1630673201923,"user_tz":-60,"elapsed":28,"user":{"displayName":"varma","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg0mykgB2BlfeZ8lmORSdRPDRvODaQ8LpLMBHSi3Q=s64","userId":"17507925689662955290"}},"outputId":"573cce62-d645-4299-a80d-c5435342d773"},"source":["acc = accuracy_score(df_test_pred['gold_label'],pred)\n","acc"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["0.43134328358208956"]},"metadata":{},"execution_count":38}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"PKrD1nRDkN0c","executionInfo":{"status":"ok","timestamp":1630673201923,"user_tz":-60,"elapsed":25,"user":{"displayName":"varma","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg0mykgB2BlfeZ8lmORSdRPDRvODaQ8LpLMBHSi3Q=s64","userId":"17507925689662955290"}},"outputId":"8f33b370-e600-491e-c903-162fc89c5aab"},"source":["cm = pd.DataFrame(confusion_matrix(df_test_pred['gold_label'],pred), columns=['contradiction','entailment', 'neutral'], \n","                  index=['contradiction', 'entailment', 'neutral'])\n","print(cm)"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["               contradiction  entailment  neutral\n","contradiction              2         249       21\n","entailment                 0         279        8\n","neutral                    1         102        8\n"]}]},{"cell_type":"code","metadata":{"id":"id6XO-q0kN0c"},"source":["#for i,j in zip(list(df_test_pred['gold_label']), pred):\n","#  print(i,j)\n","\n","pred_data = []\n","for index, (first, second) in enumerate(zip(list(df_test_pred['gold_label']), pred)):\n","    if first != second:\n","        pred_data.append([df_test_pred['sentence1'][index],df_test_pred['sentence2'][index],df_test_pred['gold_label'][index], pred[index]])\n","\n","df_mis = pd.DataFrame(pred_data, columns=['sent1', 'sent2', 'gold_lbl', 'pred_lbl'])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":417},"id":"sIcnAhQnkN0d","executionInfo":{"status":"ok","timestamp":1630673201925,"user_tz":-60,"elapsed":22,"user":{"displayName":"varma","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg0mykgB2BlfeZ8lmORSdRPDRvODaQ8LpLMBHSi3Q=s64","userId":"17507925689662955290"}},"outputId":"4aa05cd9-c1f0-4e33-a62f-2eb2fa5b34e0"},"source":["df_mis"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>sent1</th>\n","      <th>sent2</th>\n","      <th>gold_lbl</th>\n","      <th>pred_lbl</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>Things that happened almost a year ago is anci...</td>\n","      <td>Ancient history is always literally a history ...</td>\n","      <td>contradiction</td>\n","      <td>entailment</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>The days of triple-digit growth now seem like ...</td>\n","      <td>Ancient history is always literally a history ...</td>\n","      <td>contradiction</td>\n","      <td>entailment</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>So... be brave, be ready to change your views....</td>\n","      <td>Ancient history is always literally a history ...</td>\n","      <td>contradiction</td>\n","      <td>entailment</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>One month later still another  bad hat came al...</td>\n","      <td>A bad hat is always literally a hat that is bad.</td>\n","      <td>contradiction</td>\n","      <td>entailment</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>The jury heard the evidence presented, that he...</td>\n","      <td>A bad hat is always literally a hat that is bad.</td>\n","      <td>contradiction</td>\n","      <td>entailment</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>376</th>\n","      <td>This is seen as an acid test of the Government...</td>\n","      <td>An acid test is always literally a test that i...</td>\n","      <td>contradiction</td>\n","      <td>entailment</td>\n","    </tr>\n","    <tr>\n","      <th>377</th>\n","      <td>are never implemented . Redistribution of cent...</td>\n","      <td>An acid test is always literally a test that i...</td>\n","      <td>contradiction</td>\n","      <td>entailment</td>\n","    </tr>\n","    <tr>\n","      <th>378</th>\n","      <td>moment for him and I felt he came out of it ve...</td>\n","      <td>An acid test is always literally a test that i...</td>\n","      <td>contradiction</td>\n","      <td>entailment</td>\n","    </tr>\n","    <tr>\n","      <th>379</th>\n","      <td>such markers ' whilst being ungodly even , unc...</td>\n","      <td>An acid test is always literally a test that i...</td>\n","      <td>contradiction</td>\n","      <td>entailment</td>\n","    </tr>\n","    <tr>\n","      <th>380</th>\n","      <td>on the fate of the Acle Straight within weeks ...</td>\n","      <td>An acid test is always literally a test that i...</td>\n","      <td>contradiction</td>\n","      <td>entailment</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>381 rows × 4 columns</p>\n","</div>"],"text/plain":["                                                 sent1  ...    pred_lbl\n","0    Things that happened almost a year ago is anci...  ...  entailment\n","1    The days of triple-digit growth now seem like ...  ...  entailment\n","2    So... be brave, be ready to change your views....  ...  entailment\n","3    One month later still another  bad hat came al...  ...  entailment\n","4    The jury heard the evidence presented, that he...  ...  entailment\n","..                                                 ...  ...         ...\n","376  This is seen as an acid test of the Government...  ...  entailment\n","377  are never implemented . Redistribution of cent...  ...  entailment\n","378  moment for him and I felt he came out of it ve...  ...  entailment\n","379  such markers ' whilst being ungodly even , unc...  ...  entailment\n","380  on the fate of the Acle Straight within weeks ...  ...  entailment\n","\n","[381 rows x 4 columns]"]},"metadata":{},"execution_count":41}]},{"cell_type":"markdown","metadata":{"id":"QFyR-YIAF3YM"},"source":["# References"]},{"cell_type":"markdown","metadata":{"id":"hT5ntNhGGN_G"},"source":["1. https://github.com/codemunic/Natural-Language-Inference\n","\n","2. Devlin, J., Chang, M.-W., Lee, K., and Toutanova, K.Bert: Pre-training of deepbidirectional transformers for language understanding.arXiv preprint arXiv:1810.04805(2018).\n","\n","3. https://github.com/bentrevett/pytorch-sentiment-analysis"]}]}